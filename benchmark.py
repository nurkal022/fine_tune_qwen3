"""
–ü–æ–¥—Ä–æ–±–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ Qwen3-14B
–°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å —ç—Ç–∞–ª–æ–Ω–Ω—ã–º–∏ –æ—Ç–≤–µ—Ç–∞–º–∏ –∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–∞
"""
import json
import time
import torch
import random
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Tuple
from collections import defaultdict

from unsloth import FastLanguageModel
from datasets import load_dataset
from tqdm import tqdm

# –ú–µ—Ç—Ä–∏–∫–∏
try:
    from rouge_score import rouge_scorer
    ROUGE_AVAILABLE = True
except ImportError:
    ROUGE_AVAILABLE = False
    print("‚ö†Ô∏è  rouge_score –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏: pip install rouge-score")

try:
    from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction
    import nltk
    try:
        nltk.data.find('tokenizers/punkt')
    except:
        nltk.download('punkt', quiet=True)
    BLEU_AVAILABLE = True
except ImportError:
    BLEU_AVAILABLE = False
    print("‚ö†Ô∏è  nltk –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏: pip install nltk")

# ============== –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø ==============

# –ü—É—Ç—å –∫ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
MODEL_PATH = "finetuned_qwen3_8b"  # –û–±—É—á–µ–Ω–Ω–∞—è 8B –º–æ–¥–µ–ª—å

# –î–∞—Ç–∞—Å–µ—Ç –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
VAL_FILE = "combined_data/validation.jsonl"

# –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–º–µ—Ä–æ–≤ –¥–ª—è —Ç–µ—Å—Ç–∞ (None = –≤—Å–µ)
NUM_SAMPLES = 100  # –£–º–µ–Ω—å—à–µ–Ω–æ –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏, –ø–æ—Å—Ç–∞–≤—å None –¥–ª—è –ø–æ–ª–Ω–æ–≥–æ —Ç–µ—Å—Ç–∞

# –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
MAX_NEW_TOKENS = 512
TEMPERATURE = 0.1  # –ù–∏–∑–∫–∞—è —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –¥–ª—è –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤
TOP_P = 0.9

# –ü—Ä–æ–º–ø—Ç —à–∞–±–ª–æ–Ω (–¥–æ–ª–∂–µ–Ω —Å–æ–≤–ø–∞–¥–∞—Ç—å —Å –æ–±—É—á–µ–Ω–∏–µ–º)
ALPACA_PROMPT = """Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.

### Instruction:
{}

### Input:
{}

### Response:
{}"""

# ============== –ú–ï–¢–†–ò–ö–ò ==============

def calculate_exact_match(pred: str, reference: str) -> float:
    """–¢–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ (–Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–µ)"""
    pred_norm = pred.strip().lower()
    ref_norm = reference.strip().lower()
    return 1.0 if pred_norm == ref_norm else 0.0

def calculate_token_overlap(pred: str, reference: str) -> float:
    """F1 score –ø–æ —Ç–æ–∫–µ–Ω–∞–º (—Å–ª–æ–≤–∞)"""
    pred_tokens = set(pred.lower().split())
    ref_tokens = set(reference.lower().split())
    
    if not pred_tokens or not ref_tokens:
        return 0.0
    
    common = pred_tokens & ref_tokens
    
    if len(common) == 0:
        return 0.0
    
    precision = len(common) / len(pred_tokens)
    recall = len(common) / len(ref_tokens)
    
    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0
    return f1

def calculate_rouge(pred: str, reference: str, scorer) -> Dict[str, float]:
    """ROUGE scores"""
    scores = scorer.score(reference, pred)
    return {
        'rouge1': scores['rouge1'].fmeasure,
        'rouge2': scores['rouge2'].fmeasure,
        'rougeL': scores['rougeL'].fmeasure,
    }

def calculate_bleu(pred: str, reference: str) -> float:
    """BLEU score"""
    pred_tokens = pred.lower().split()
    ref_tokens = [reference.lower().split()]
    
    if not pred_tokens or not ref_tokens[0]:
        return 0.0
    
    smoothing = SmoothingFunction().method1
    try:
        score = sentence_bleu(ref_tokens, pred_tokens, smoothing_function=smoothing)
    except:
        score = 0.0
    return score

def calculate_length_ratio(pred: str, reference: str) -> float:
    """–°–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ –¥–ª–∏–Ω (–±–ª–∏–∂–µ –∫ 1 = –ª—É—á—à–µ)"""
    pred_len = len(pred.split())
    ref_len = len(reference.split())
    
    if ref_len == 0:
        return 0.0
    
    ratio = pred_len / ref_len
    # –®—Ç—Ä–∞—Ñ –∑–∞ —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ –∏–ª–∏ –¥–ª–∏–Ω–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã
    if ratio > 1:
        return 1 / ratio
    return ratio

def calculate_contains_key_info(pred: str, reference: str) -> float:
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –∫–ª—é—á–µ–≤–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏"""
    # –ò–∑–≤–ª–µ–∫–∞–µ–º —á–∏—Å–ª–∞, –¥–∞—Ç—ã, –∫–ª—é—á–µ–≤—ã–µ —Ç–µ—Ä–º–∏–Ω—ã –∏–∑ reference
    import re
    
    # –ß–∏—Å–ª–∞
    ref_numbers = set(re.findall(r'\d+', reference))
    pred_numbers = set(re.findall(r'\d+', pred))
    
    if ref_numbers:
        number_overlap = len(ref_numbers & pred_numbers) / len(ref_numbers)
    else:
        number_overlap = 1.0
    
    # –ö–ª—é—á–µ–≤—ã–µ —é—Ä–∏–¥–∏—á–µ—Å–∫–∏–µ —Ç–µ—Ä–º–∏–Ω—ã (–Ω–∞ —Ä—É—Å—Å–∫–æ–º)
    legal_terms = [
        '—Å—Ç–∞—Ç—å—è', '–∑–∞–∫–æ–Ω', '–∫–æ–¥–µ–∫—Å', '–ø—Ä–∞–≤–æ', '–æ–±—è–∑–∞–Ω–Ω–æ—Å—Ç—å', '–¥–æ–≥–æ–≤–æ—Ä',
        '—Å—Ä–æ–∫', '—à—Ç—Ä–∞—Ñ', '—Å—É–¥', '–∏—Å–∫', '–∑–∞—è–≤–ª–µ–Ω–∏–µ', '–¥–æ–∫—É–º–µ–Ω—Ç', '–ª–∏—Ü–æ',
        '–≥—Ä–∞–∂–¥–∞–Ω–∏–Ω', '–æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è', '–≥–æ—Å—É–¥–∞—Ä—Å—Ç–≤–æ', '–æ—Ä–≥–∞–Ω', '—Ä–µ—à–µ–Ω–∏–µ'
    ]
    
    ref_lower = reference.lower()
    pred_lower = pred.lower()
    
    ref_terms = [t for t in legal_terms if t in ref_lower]
    if ref_terms:
        term_overlap = sum(1 for t in ref_terms if t in pred_lower) / len(ref_terms)
    else:
        term_overlap = 1.0
    
    return (number_overlap + term_overlap) / 2

# ============== –û–°–ù–û–í–ù–´–ï –§–£–ù–ö–¶–ò–ò ==============

def load_model(model_path: str):
    """–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏"""
    print(f"üîÑ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏: {model_path}")
    
    model, tokenizer = FastLanguageModel.from_pretrained(
        model_name=model_path,
        max_seq_length=2048,
        dtype=None,
        load_in_4bit=True,  # 4-bit –¥–ª—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ (–±—ã—Å—Ç—Ä–µ–µ)
    )
    
    FastLanguageModel.for_inference(model)
    print("‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
    
    return model, tokenizer

def load_test_data(val_file: str, num_samples: int = None) -> List[Dict]:
    """–ó–∞–≥—Ä—É–∑–∫–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
    print(f"üìä –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö: {val_file}")
    
    data = []
    with open(val_file, 'r', encoding='utf-8') as f:
        for line in f:
            if line.strip():
                data.append(json.loads(line))
    
    print(f"   –í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π: {len(data)}")
    
    if num_samples and num_samples < len(data):
        random.seed(42)  # –î–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏
        data = random.sample(data, num_samples)
        print(f"   –í—ã–±—Ä–∞–Ω–æ –¥–ª—è —Ç–µ—Å—Ç–∞: {num_samples}")
    
    return data

def generate_response(model, tokenizer, instruction: str, input_text: str) -> Tuple[str, float]:
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞"""
    prompt = ALPACA_PROMPT.format(instruction, input_text, "")
    inputs = tokenizer([prompt], return_tensors="pt").to("cuda")
    
    start_time = time.time()
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=MAX_NEW_TOKENS,
            temperature=TEMPERATURE,
            top_p=TOP_P,
            use_cache=True,
            pad_token_id=tokenizer.pad_token_id,
        )
    
    gen_time = time.time() - start_time
    
    # –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–æ–ª—å–∫–æ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–π —á–∞—Å—Ç–∏
    response = tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)
    
    return response.strip(), gen_time

def run_benchmark(model, tokenizer, test_data: List[Dict]) -> Dict:
    """–ó–∞–ø—É—Å–∫ –±–µ–Ω—á–º–∞—Ä–∫–∞"""
    print("\n" + "=" * 60)
    print("üöÄ –ó–ê–ü–£–°–ö –ë–ï–ù–ß–ú–ê–†–ö–ê")
    print("=" * 60)
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–∫–æ—Ä–µ—Ä–æ–≤
    rouge = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=False) if ROUGE_AVAILABLE else None
    
    results = []
    metrics_sum = defaultdict(float)
    
    total_gen_time = 0
    
    for i, sample in enumerate(tqdm(test_data, desc="–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ")):
        instruction = sample['instruction']
        input_text = sample.get('input', '')
        reference = sample['output']
        
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è
        prediction, gen_time = generate_response(model, tokenizer, instruction, input_text)
        total_gen_time += gen_time
        
        # –†–∞—Å—á—ë—Ç –º–µ—Ç—Ä–∏–∫
        metrics = {
            'exact_match': calculate_exact_match(prediction, reference),
            'token_f1': calculate_token_overlap(prediction, reference),
            'length_ratio': calculate_length_ratio(prediction, reference),
            'key_info': calculate_contains_key_info(prediction, reference),
        }
        
        if ROUGE_AVAILABLE:
            rouge_scores = calculate_rouge(prediction, reference, rouge)
            metrics.update(rouge_scores)
        
        if BLEU_AVAILABLE:
            metrics['bleu'] = calculate_bleu(prediction, reference)
        
        metrics['gen_time'] = gen_time
        
        # –°—É–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è —Å—Ä–µ–¥–Ω–∏—Ö
        for k, v in metrics.items():
            metrics_sum[k] += v
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
        results.append({
            'id': i,
            'instruction': instruction[:100] + '...' if len(instruction) > 100 else instruction,
            'input': input_text[:50] + '...' if len(input_text) > 50 else input_text,
            'reference': reference[:200] + '...' if len(reference) > 200 else reference,
            'prediction': prediction[:200] + '...' if len(prediction) > 200 else prediction,
            'metrics': metrics,
        })
    
    # –†–∞—Å—á—ë—Ç —Å—Ä–µ–¥–Ω–∏—Ö
    num_samples = len(test_data)
    avg_metrics = {k: v / num_samples for k, v in metrics_sum.items()}
    
    return {
        'num_samples': num_samples,
        'total_time': total_gen_time,
        'avg_time_per_sample': total_gen_time / num_samples,
        'avg_metrics': avg_metrics,
        'results': results,
    }

def print_results(benchmark_results: Dict):
    """–í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""
    print("\n" + "=" * 60)
    print("üìä –†–ï–ó–£–õ–¨–¢–ê–¢–´ –ë–ï–ù–ß–ú–ê–†–ö–ê")
    print("=" * 60)
    
    num_samples = benchmark_results['num_samples']
    total_time = benchmark_results['total_time']
    avg_metrics = benchmark_results['avg_metrics']
    
    print(f"\nüìà –û–ë–©–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê:")
    print(f"   –ü—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–æ –ø—Ä–∏–º–µ—Ä–æ–≤: {num_samples}")
    print(f"   –û–±—â–µ–µ –≤—Ä–µ–º—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {total_time:.1f} —Å–µ–∫")
    print(f"   –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –Ω–∞ –ø—Ä–∏–º–µ—Ä: {total_time/num_samples:.2f} —Å–µ–∫")
    print(f"   –ü—Ä–∏–º–µ—Ä–æ–≤ –≤ –º–∏–Ω—É—Ç—É: {num_samples / (total_time / 60):.1f}")
    
    print(f"\nüìä –ú–ï–¢–†–ò–ö–ò –ö–ê–ß–ï–°–¢–í–ê:")
    print("-" * 40)
    
    # –û—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
    print(f"   Token F1:        {avg_metrics['token_f1']*100:.2f}%")
    print(f"   Exact Match:     {avg_metrics['exact_match']*100:.2f}%")
    print(f"   Key Info Score:  {avg_metrics['key_info']*100:.2f}%")
    print(f"   Length Ratio:    {avg_metrics['length_ratio']*100:.2f}%")
    
    if ROUGE_AVAILABLE:
        print("-" * 40)
        print(f"   ROUGE-1:         {avg_metrics['rouge1']*100:.2f}%")
        print(f"   ROUGE-2:         {avg_metrics['rouge2']*100:.2f}%")
        print(f"   ROUGE-L:         {avg_metrics['rougeL']*100:.2f}%")
    
    if BLEU_AVAILABLE:
        print("-" * 40)
        print(f"   BLEU:            {avg_metrics['bleu']*100:.2f}%")
    
    # –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    print("\nüìù –ò–ù–¢–ï–†–ü–†–ï–¢–ê–¶–ò–Ø:")
    print("-" * 40)
    
    overall_score = (
        avg_metrics['token_f1'] * 0.3 +
        avg_metrics['key_info'] * 0.3 +
        avg_metrics.get('rougeL', avg_metrics['token_f1']) * 0.2 +
        avg_metrics['length_ratio'] * 0.2
    )
    
    print(f"   –û–±—â–∏–π –±–∞–ª–ª: {overall_score*100:.1f}%")
    
    if overall_score >= 0.7:
        print("   ‚úÖ –û—Ç–ª–∏—á–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ! –ú–æ–¥–µ–ª—å —Ö–æ—Ä–æ—à–æ –æ–±—É—á–µ–Ω–∞.")
    elif overall_score >= 0.5:
        print("   ‚ö†Ô∏è  –•–æ—Ä–æ—à–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ. –í–æ–∑–º–æ–∂–Ω—ã —É–ª—É—á—à–µ–Ω–∏—è.")
    elif overall_score >= 0.3:
        print("   ‚ö†Ô∏è  –°—Ä–µ–¥–Ω–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ. –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –¥–æ–æ–±—É—á–µ–Ω–∏–µ.")
    else:
        print("   ‚ùå –ù–∏–∑–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ. –¢—Ä–µ–±—É–µ—Ç—Å—è –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ.")
    
    print("=" * 60)

def save_results(benchmark_results: Dict, output_file: str = None):
    """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ —Ñ–∞–π–ª"""
    if output_file is None:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_file = f"benchmark_results_{timestamp}.json"
    
    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
    save_data = {
        'model_path': MODEL_PATH,
        'num_samples': benchmark_results['num_samples'],
        'total_time': benchmark_results['total_time'],
        'avg_metrics': benchmark_results['avg_metrics'],
        'timestamp': datetime.now().isoformat(),
        'config': {
            'max_new_tokens': MAX_NEW_TOKENS,
            'temperature': TEMPERATURE,
            'top_p': TOP_P,
        },
        # –î–µ—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        'detailed_results': benchmark_results['results'][:20],  # –ü–µ—Ä–≤—ã–µ 20 –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –º–µ—Å—Ç–∞
    }
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(save_data, f, ensure_ascii=False, indent=2)
    
    print(f"\nüíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {output_file}")

def show_examples(benchmark_results: Dict, num_examples: int = 5):
    """–ü–æ–∫–∞–∑–∞—Ç—å –ø—Ä–∏–º–µ—Ä—ã –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π"""
    print("\n" + "=" * 60)
    print("üìù –ü–†–ò–ú–ï–†–´ –ü–†–ï–î–°–ö–ê–ó–ê–ù–ò–ô")
    print("=" * 60)
    
    results = benchmark_results['results']
    
    # –°–ª—É—á–∞–π–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã
    examples = random.sample(results, min(num_examples, len(results)))
    
    for i, ex in enumerate(examples, 1):
        print(f"\n--- –ü—Ä–∏–º–µ—Ä {i} ---")
        print(f"üìù Instruction: {ex['instruction']}")
        if ex['input']:
            print(f"üì• Input: {ex['input']}")
        print(f"\n‚úÖ –≠—Ç–∞–ª–æ–Ω:")
        print(f"   {ex['reference']}")
        print(f"\nü§ñ –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ:")
        print(f"   {ex['prediction']}")
        print(f"\nüìä –ú–µ—Ç—Ä–∏–∫–∏:")
        print(f"   Token F1: {ex['metrics']['token_f1']*100:.1f}%")
        print(f"   Key Info: {ex['metrics']['key_info']*100:.1f}%")
        if 'rougeL' in ex['metrics']:
            print(f"   ROUGE-L: {ex['metrics']['rougeL']*100:.1f}%")
        print("-" * 40)

def show_worst_examples(benchmark_results: Dict, num_examples: int = 5):
    """–ü–æ–∫–∞–∑–∞—Ç—å —Ö—É–¥—à–∏–µ –ø—Ä–∏–º–µ—Ä—ã (–¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –æ—à–∏–±–æ–∫)"""
    print("\n" + "=" * 60)
    print("‚ùå –•–£–î–®–ò–ï –ü–†–ï–î–°–ö–ê–ó–ê–ù–ò–Ø (–¥–ª—è –∞–Ω–∞–ª–∏–∑–∞)")
    print("=" * 60)
    
    results = benchmark_results['results']
    
    # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ token_f1
    sorted_results = sorted(results, key=lambda x: x['metrics']['token_f1'])
    worst = sorted_results[:num_examples]
    
    for i, ex in enumerate(worst, 1):
        print(f"\n--- –•—É–¥—à–∏–π –ø—Ä–∏–º–µ—Ä {i} ---")
        print(f"üìù Instruction: {ex['instruction']}")
        if ex['input']:
            print(f"üì• Input: {ex['input']}")
        print(f"\n‚úÖ –≠—Ç–∞–ª–æ–Ω:")
        print(f"   {ex['reference']}")
        print(f"\nü§ñ –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ:")
        print(f"   {ex['prediction']}")
        print(f"\nüìä Token F1: {ex['metrics']['token_f1']*100:.1f}%")
        print("-" * 40)

def main():
    print("=" * 60)
    print("üß™ –ë–ï–ù–ß–ú–ê–†–ö –ú–û–î–ï–õ–ò QWEN3-14B")
    print("=" * 60)
    print(f"–ú–æ–¥–µ–ª—å: {MODEL_PATH}")
    print(f"–î–∞—Ç–∞—Å–µ—Ç: {VAL_FILE}")
    print(f"–ü—Ä–∏–º–µ—Ä–æ–≤: {NUM_SAMPLES or '–≤—Å–µ'}")
    print("=" * 60)
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –º–æ–¥–µ–ª–∏
    if not Path(MODEL_PATH).exists():
        print(f"‚ùå –ú–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {MODEL_PATH}")
        print("   –£–∫–∞–∂–∏ –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π –ø—É—Ç—å –≤ MODEL_PATH")
        return
    
    # –ó–∞–≥—Ä—É–∑–∫–∞
    model, tokenizer = load_model(MODEL_PATH)
    test_data = load_test_data(VAL_FILE, NUM_SAMPLES)
    
    # –ë–µ–Ω—á–º–∞—Ä–∫
    results = run_benchmark(model, tokenizer, test_data)
    
    # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã
    print_results(results)
    show_examples(results, num_examples=3)
    show_worst_examples(results, num_examples=3)
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
    save_results(results)
    
    print("\n‚úÖ –ë–µ–Ω—á–º–∞—Ä–∫ –∑–∞–≤–µ—Ä—à—ë–Ω!")

if __name__ == "__main__":
    main()


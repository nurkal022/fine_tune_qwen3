"""
Live Chat —Å –æ–±—É—á–µ–Ω–Ω–æ–π Qwen3-14B (16-bit) –º–æ–¥–µ–ª—å—é
–ö–∞–∑–∞—Ö—Å—Ç–∞–Ω—Å–∫–æ–µ –∑–∞–∫–æ–Ω–æ–¥–∞—Ç–µ–ª—å—Å—Ç–≤–æ
"""
import torch
from unsloth import FastLanguageModel
from transformers import TextStreamer

# ============== –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø ==============
MODEL_PATH = "outputs_14b_16bit/checkpoint-21303"
MAX_SEQ_LENGTH = 2048
LOAD_IN_4BIT = True  # –î–ª—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ –º–æ–∂–Ω–æ –≤ 4-bit (—ç–∫–æ–Ω–æ–º–∏—Ç –ø–∞–º—è—Ç—å)

# Prompt template (Alpaca format)
ALPACA_PROMPT = """Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.

### Instruction:
{}

### Input:
{}

### Response:
{}"""

def load_model():
    """–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏"""
    print("=" * 60)
    print("üöÄ –ó–∞–≥—Ä—É–∑–∫–∞ Qwen3-14B (fine-tuned –¥–ª—è –ö–ó –∑–∞–∫–æ–Ω–æ–¥–∞—Ç–µ–ª—å—Å—Ç–≤–∞)...")
    print("=" * 60)
    
    model, tokenizer = FastLanguageModel.from_pretrained(
        model_name=MODEL_PATH,
        max_seq_length=MAX_SEQ_LENGTH,
        dtype=None,
        load_in_4bit=LOAD_IN_4BIT,
    )
    FastLanguageModel.for_inference(model)
    
    # GPU info
    if torch.cuda.is_available():
        allocated = torch.cuda.memory_allocated() / 1024**3
        print(f"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ | GPU –ø–∞–º—è—Ç—å: {allocated:.1f} GB")
    
    return model, tokenizer

def generate_response(model, tokenizer, instruction: str, input_text: str = "", 
                     max_tokens: int = 1024, temperature: float = 0.7, 
                     top_p: float = 0.9, stream: bool = True):
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞"""
    prompt = ALPACA_PROMPT.format(instruction, input_text, "")
    inputs = tokenizer([prompt], return_tensors="pt").to("cuda")
    
    if stream:
        streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)
        output = model.generate(
            **inputs,
            streamer=streamer,
            max_new_tokens=max_tokens,
            use_cache=True,
            temperature=temperature,
            top_p=top_p,
            do_sample=True,
            pad_token_id=tokenizer.eos_token_id,
        )
    else:
        output = model.generate(
            **inputs,
            max_new_tokens=max_tokens,
            use_cache=True,
            temperature=temperature,
            top_p=top_p,
            do_sample=True,
            pad_token_id=tokenizer.eos_token_id,
        )
        response = tokenizer.decode(output[0], skip_special_tokens=True)
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–æ–ª—å–∫–æ –æ—Ç–≤–µ—Ç
        if "### Response:" in response:
            response = response.split("### Response:")[-1].strip()
        print(response)
    
    return output

def run_tests(model, tokenizer):
    """–ó–∞–ø—É—Å–∫ —Ç–µ—Å—Ç–æ–≤—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤"""
    tests = [
        {
            "title": "–î–æ–≥–æ–≤–æ—Ä –ì–ü–•",
            "instruction": "–û—Ç–≤–µ—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å –ø–æ –∫–∞–∑–∞—Ö—Å—Ç–∞–Ω—Å–∫–æ–º—É –∑–∞–∫–æ–Ω–æ–¥–∞—Ç–µ–ª—å—Å—Ç–≤—É.",
            "input": "–ú–æ–∂–Ω–æ –ª–∏ –∑–∞–∫–ª—é—á–∏—Ç—å –¥–æ–≥–æ–≤–æ—Ä –ì–ü–• —Å —É–∫–∞–∑–∞–Ω–∏–µ–º —Ü–µ–Ω—ã —Ä–∞–±–æ—Ç—ã –∑–∞ —Å—É—Ç–∫–∏?"
        },
        {
            "title": "–ó–µ–º–µ–ª—å–Ω–æ–µ –ø—Ä–∞–≤–æ",
            "instruction": "–ö–∞–∫ —Ä–µ–≥—É–ª–∏—Ä—É–µ—Ç—Å—è –æ–±—Ä–∞—â–µ–Ω–∏–µ –≤–∑—ã—Å–∫–∞–Ω–∏—è –Ω–∞ –∑–µ–º–µ–ª—å–Ω—ã–µ —É—á–∞—Å—Ç–∫–∏ –≤ –†–ö?",
            "input": ""
        },
        {
            "title": "–¢—Ä—É–¥–æ–≤–æ–µ –ø—Ä–∞–≤–æ",
            "instruction": "–û—Ç–≤–µ—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å –ø–æ –∫–∞–∑–∞—Ö—Å—Ç–∞–Ω—Å–∫–æ–º—É –∑–∞–∫–æ–Ω–æ–¥–∞—Ç–µ–ª—å—Å—Ç–≤—É.",
            "input": "–ò–º–µ–µ—Ç –ª–∏ –ø—Ä–∞–≤–æ —Ä–∞–±–æ—Ç–Ω–∏–∫ –Ω–∞—Ö–æ–¥–∏—Ç—å—Å—è –Ω–∞ –±–æ–ª—å–Ω–∏—á–Ω–æ–º –±–æ–ª–µ–µ 60 –¥–Ω–µ–π?"
        },
        {
            "title": "–ù–∞–ª–æ–≥–æ–≤–æ–µ –ø—Ä–∞–≤–æ", 
            "instruction": "–ö–∞–∫–∏–µ –ø—Ä–∞–≤–∞ –∏–º–µ—é—Ç –ø—Ä–æ–≤–µ—Ä—è–µ–º—ã–µ –ª–∏—Ü–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ–¥–µ–Ω–∏–∏ –Ω–∞–ª–æ–≥–æ–≤–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏?",
            "input": ""
        },
    ]
    
    for i, test in enumerate(tests, 1):
        print(f"\n{'='*60}")
        print(f"üìã –¢–ï–°–¢ {i}: {test['title']}")
        print(f"{'='*60}")
        print(f"‚ùì Instruction: {test['instruction']}")
        if test['input']:
            print(f"üì• Input: {test['input']}")
        print(f"\nüí¨ Response:")
        print("-" * 40)
        generate_response(model, tokenizer, test['instruction'], test['input'])
        print("-" * 40)

def chat_mode(model, tokenizer):
    """–ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π —á–∞—Ç"""
    print("\n" + "=" * 60)
    print("üí¨ –ò–ù–¢–ï–†–ê–ö–¢–ò–í–ù–´–ô –ß–ê–¢")
    print("=" * 60)
    print("–ö–æ–º–∞–Ω–¥—ã:")
    print("  exit, quit, q  - –≤—ã—Ö–æ–¥")
    print("  clear, cls     - –æ—á–∏—Å—Ç–∏—Ç—å —ç–∫—Ä–∞–Ω")
    print("  test           - –∑–∞–ø—É—Å—Ç–∏—Ç—å —Ç–µ—Å—Ç—ã")
    print("  temp <0.1-2.0> - –∏–∑–º–µ–Ω–∏—Ç—å temperature")
    print("=" * 60)
    
    temperature = 0.7
    
    while True:
        print()
        try:
            user_input = input("üßë –í—ã: ").strip()
        except (KeyboardInterrupt, EOFError):
            print("\nüëã –î–æ —Å–≤–∏–¥–∞–Ω–∏—è!")
            break
        
        if not user_input:
            continue
        
        # –ö–æ–º–∞–Ω–¥—ã
        if user_input.lower() in ['exit', 'quit', 'q']:
            print("üëã –î–æ —Å–≤–∏–¥–∞–Ω–∏—è!")
            break
        
        if user_input.lower() in ['clear', 'cls']:
            print("\033[2J\033[H", end="")  # Clear terminal
            continue
        
        if user_input.lower() == 'test':
            run_tests(model, tokenizer)
            continue
        
        if user_input.lower().startswith('temp '):
            try:
                temperature = float(user_input.split()[1])
                temperature = max(0.1, min(2.0, temperature))
                print(f"‚úÖ Temperature —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω: {temperature}")
            except:
                print("‚ùå –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ: temp <0.1-2.0>")
            continue
        
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞
        print(f"\nü§ñ –ê—Å—Å–∏—Å—Ç–µ–Ω—Ç (temp={temperature}):")
        print("-" * 40)
        generate_response(
            model, tokenizer,
            instruction="–û—Ç–≤–µ—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å –ø–æ –∫–∞–∑–∞—Ö—Å—Ç–∞–Ω—Å–∫–æ–º—É –∑–∞–∫–æ–Ω–æ–¥–∞—Ç–µ–ª—å—Å—Ç–≤—É.",
            input_text=user_input,
            temperature=temperature
        )
        print("-" * 40)

def main():
    model, tokenizer = load_model()
    
    # –í—ã–±–æ—Ä —Ä–µ–∂–∏–º–∞
    print("\n" + "=" * 60)
    print("–í—ã–±–µ—Ä–∏—Ç–µ —Ä–µ–∂–∏–º:")
    print("  1. –ó–∞–ø—É—Å—Ç–∏—Ç—å —Ç–µ—Å—Ç—ã")
    print("  2. –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π —á–∞—Ç")
    print("  3. –û–±–∞ (—Ç–µ—Å—Ç—ã + —á–∞—Ç)")
    print("=" * 60)
    
    try:
        choice = input("–í—ã–±–æ—Ä [3]: ").strip() or "3"
    except (KeyboardInterrupt, EOFError):
        choice = "3"
    
    if choice in ["1", "3"]:
        run_tests(model, tokenizer)
    
    if choice in ["2", "3"]:
        chat_mode(model, tokenizer)

if __name__ == "__main__":
    main()

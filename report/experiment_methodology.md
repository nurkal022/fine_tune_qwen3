# Методология и ход экспериментов

## Общая информация

- **Проект**: Fine-tuning Qwen3 для казахстанского правового домена
- **Цель**: Создать LLM-ассистента по законодательству Республики Казахстан
- **Период проведения**: Январь — Февраль 2026
- **Оборудование**:
  - Локальная машина: NVIDIA GeForce RTX 5080 Laptop GPU (15.5 GB VRAM), Linux
  - Сервер: 2x NVIDIA GeForce RTX 5090 (32 GB VRAM каждая), Linux

---

## 1. Подготовка данных

### 1.1 Источники данных
Датасет собран из открытых источников юридических консультаций по казахстанскому законодательству. Каждый пример представляет собой пару "вопрос — ответ" в формате Alpaca (instruction/input/output).

### 1.2 Статистика датасета

| Параметр | Значение |
|----------|----------|
| Обучающая выборка | 56,802 Q&A пар |
| Валидационная выборка | 6,312 Q&A пар |
| Формат | JSONL (instruction, input, output) |
| Языки | Русский — 76.2%, Казахский — 23.8% |

**Распределение по правовым доменам (train)**:

| Домен | Количество | Доля |
|-------|-----------|------|
| other (общие вопросы) | 18,295 | 32.2% |
| civil (гражданское) | 12,818 | 22.6% |
| tax (налоговое) | 10,302 | 18.1% |
| labor (трудовое) | 10,277 | 18.1% |
| business (предпринимательское) | 10,234 | 18.0% |
| land (земельное) | 5,281 | 9.3% |
| administrative | 3,938 | 6.9% |
| constitutional | 2,989 | 5.3% |
| criminal (уголовное) | 2,522 | 4.4% |
| housing (жилищное) | 1,595 | 2.8% |
| family (семейное) | 937 | 1.6% |

**Средняя длина текстов (слова)**:

| Поле | Mean | Median | Max |
|------|------|--------|-----|
| instruction | 8.5 | 7 | 48 |
| input | 13.4 | 0 | 882 |
| output | 49.5 | 23 | 2,417 |

---

## 2. Обучение моделей

### 2.1 Базовые модели
Использованы три размера модели Qwen3 через Unsloth в 4-bit квантизации:

| Модель | Параметры | HuggingFace ID |
|--------|-----------|---------------|
| Qwen3-4B | 4 млрд | `unsloth/Qwen3-4B-unsloth-bnb-4bit` |
| Qwen3-8B | 8 млрд | `unsloth/Qwen3-8B-unsloth-bnb-4bit` |
| Qwen3-14B | 14.8 млрд | `unsloth/Qwen3-14B-unsloth-bnb-4bit` |

### 2.2 Конфигурация обучения

Все три модели обучены с идентичными гиперпараметрами для честного сравнения:

| Параметр | Значение |
|----------|----------|
| Метод | LoRA (Low-Rank Adaptation) |
| LoRA rank (r) | 16 |
| LoRA alpha | 16 |
| LoRA dropout | 0 |
| Target modules | q_proj, k_proj, v_proj, o_proj, gate_proj, up_proj, down_proj |
| Квантизация | 4-bit (BitsAndBytes через Unsloth) |
| Оптимизатор | AdamW 8-bit |
| Learning rate | 2e-4 |
| LR scheduler | Cosine |
| Warmup ratio | 0.03 |
| Эпохи | 3 |
| Max sequence length | 2048 |
| Precision | bfloat16 |
| Prompt format | Alpaca (instruction/input/response) |

**Конфигурация батчей по размерам моделей**:

| Модель | Batch size | Grad accum | Effective batch |
|--------|-----------|------------|-----------------|
| 4B | 4 | 4 | 16 |
| 8B | 2 | 4 | 8 |
| 14B | 1 | 8 | 8 |

### 2.3 Процесс обучения

**Qwen3-4B**: Обучена на RTX 5080 Laptop (15.5 GB VRAM)
- Время обучения: 15 часов 40 минут (56,398 секунд)
- 10,653 шагов (3 эпохи)
- Начальный loss: 1.20, финальный loss: 0.65
- VRAM: 7.4 GB пиковое использование

**Qwen3-8B**: Обучена ранее (Январь 2026) на аналогичной конфигурации
- Адаптер: 174 MB (adapter_model.safetensors)
- VRAM: 11.2 GB пиковое использование

**Qwen3-14B**: Обучена на RTX 5090 (32 GB VRAM)
- Время обучения: 14 часов 9 минут (50,964 секунд)
- 21,303 шагов (3 эпохи)
- Начальный loss: 1.78, финальный loss: 0.50
- Обучаемые параметры: 64.2M из 14.8B (0.43%)

### 2.4 Выходные артефакты
Обученные LoRA-адаптеры сохранены в директориях:
- `lora_qwen3_4b/` (~132 MB)
- `lora_qwen3_8b/` (~174 MB)
- `lora_qwen3_14b/` (~300 MB)

---

## 3. Система метрик

### 3.1 Автоматические метрики

| Метрика | Описание | Реализация |
|---------|----------|------------|
| **BERTScore F1** | Семантическое сходство на основе эмбеддингов | `xlm-roberta-large` (мультиязычная модель для RU/KZ) |
| **ROUGE-L** | Overlap по самой длинной общей подпоследовательности | Библиотека `rouge-score` |
| **Citation Accuracy** | Доля правильных юридических ссылок (ст., п., закон №) | Кастомная: regex extraction + set intersection |
| **Hallucination Rate** | Доля выдуманных ссылок в ответе модели | Кастомная: citations_pred - citations_ref / total_pred |
| **Key Info Score** | Overlap ключевых чисел и юридических терминов | Кастомная: числа + 18 юридических терминов |
| **Latency** | Среднее время генерации одного ответа | `time.time()` вокруг `model.generate()` |

### 3.2 Статистическая значимость

Для всех метрик вычислены **Bootstrap 95% Confidence Intervals**:
- 1000 bootstrap итераций
- Percentile method (2.5% и 97.5% перцентили)
- `np.random.seed(42)` для воспроизводимости

### 3.3 Классификация доменов и языков

**Язык**: определяется по наличию казахских символов (ә, ғ, қ, ң, ө, ү, ұ, і) → `kz`, иначе → `ru`

**Домен**: keyword matching по 10 категориям (criminal, civil, administrative, labor, tax, family, land, housing, business, constitutional), по умолчанию `other`

---

## 4. Эксперимент 1 & 2: Бенчмарк и Scaling Analysis

### 4.1 Процедура
Для каждой из 6 конфигураций (3 размера × Base/FT) запущен бенчмарк:

```
python benchmark.py --baseline "unsloth/Qwen3-4B-unsloth-bnb-4bit" --samples 500
python benchmark.py --model lora_qwen3_4b --samples 500
# ... повторить для 8B и 14B
```

- **500 samples** из валидационной выборки (random.seed(42))
- Одни и те же 500 вопросов для всех моделей
- Каждый ответ оценивается по всем метрикам

**4B и 8B** бенчмарки запущены на RTX 5080 Laptop.
**14B** бенчмарки запущены на RTX 5090 (сервер).

### 4.2 Результаты

| Модель | BERTScore F1 | Citation Acc | Halluc Rate | Key Info | Latency | VRAM |
|--------|-------------|-------------|-------------|----------|---------|------|
| 4B Base | 82.4% ± 0.3 | 73.2% ± 3.8 | 23.2% ± 3.8 | 69.4% ± 3.0 | 10.7s | 7.1G |
| **4B FT** | **89.7% ± 0.3** | **80.1% ± 3.2** | 26.7% ± 3.7 | 72.6% ± 2.6 | **2.5s** | 7.4G |
| 8B Base | 83.1% ± 0.2 | 73.3% ± 3.7 | 20.4% ± 3.7 | 72.7% ± 2.8 | 14.0s | 10.9G |
| **8B FT** | **89.8% ± 0.3** | **79.3% ± 3.1** | 28.6% ± 3.7 | 72.5% ± 2.5 | **3.5s** | 11.2G |
| 14B Base | 81.8% ± 0.2 | 72.6% ± 3.7 | 13.6% ± 3.0 | 64.9% ± 3.1 | 10.3s | 14.2G |
| **14B FT** | **90.2% ± 0.3** | **80.3% ± 3.1** | 27.4% ± 3.8 | 73.5% ± 2.6 | **3.2s** | 14.8G |

### 4.3 Наблюдения

1. Fine-tuning даёт стабильный прирост: +7-8% BERTScore, +7% Citation Accuracy
2. Confidence intervals всех FT моделей перекрываются → разница между размерами статистически незначима
3. Latency уменьшается в 3-4x после FT (модель учится генерировать более краткие и точные ответы)
4. 14B Base отвечает на английском (thinking mode Qwen3) — FT исправляет это
5. Hallucination Rate растёт после FT, потому что FT модели чаще цитируют статьи (больше ссылок → больше шанс ошибки)

---

## 5. Эксперимент 3: RAG Ablation

### 5.1 Дизайн эксперимента

**Цель**: Оценить, может ли RAG (Retrieval-Augmented Generation) заменить или дополнить fine-tuning.

**Корпус для поиска**: обучающий датасет (56,802 Q&A пар)

**Метод поиска**: TF-IDF (scikit-learn TfidfVectorizer, max_features=50000, sublinear_tf=True) + cosine similarity

**Предотвращение утечки данных**: документы с cosine similarity > 0.95 к запросу исключаются из результатов

**RAG промпт**: стандартный Alpaca + секция "Legal Context" с найденными Q&A парами

**Конфигурации**:

| Сценарий 1 (Может ли RAG заменить FT?) | Сценарий 2 (Улучшает ли RAG FT?) |
|----------------------------------------|----------------------------------|
| Base only | FT only |
| Base + RAG top-1 | FT + RAG top-1 |
| Base + RAG top-3 | FT + RAG top-3 |
| Base + RAG top-5 | FT + RAG top-5 |

Оба сценария: 200 samples, Qwen3-4B, bootstrap 95% CI, BERTScore (xlm-roberta-large)

### 5.2 Процедура

```bash
# Сценарий 1: Base + RAG
python rag_benchmark.py --baseline "unsloth/Qwen3-4B-unsloth-bnb-4bit" \
    --corpus combined_data/train.jsonl --samples 200 --top-k 1 3 5

# Сценарий 2: FT + RAG
python rag_benchmark.py --model lora_qwen3_4b \
    --corpus combined_data/train.jsonl --samples 200 --top-k 1 3 5
```

Время выполнения:
- Сценарий 1 (Base): ~2.5 часа (Base модель медленнее: ~10s/sample)
- Сценарий 2 (FT): ~40 минут (FT модель быстрее: ~2.5-3.9s/sample)

### 5.3 Результаты

**Сценарий 1: Base + RAG**

| Configuration | BERTScore | CitAcc | Halluc | KeyInfo | Latency |
|---------------|-----------|--------|--------|---------|---------|
| Base only | 82.6% | 73.5% | 21.8% | 69.5% | 9.9s |
| Base + RAG top-1 | 82.8% | 75.6% | 35.9% | 72.0% | 10.1s |
| Base + RAG top-3 | 83.1% | 78.1% | 30.9% | 72.6% | 10.8s |
| Base + RAG top-5 | 83.2% | **80.3%** | 35.9% | 73.7% | 11.4s |

**Сценарий 2: FT + RAG**

| Configuration | BERTScore | CitAcc | Halluc | KeyInfo | Latency |
|---------------|-----------|--------|--------|---------|---------|
| **FT only** | **89.8%** | **81.9%** | 26.5% | **75.8%** | **2.5s** |
| FT + RAG top-1 | 88.3% | 77.2% | 27.4% | 69.5% | 3.1s |
| FT + RAG top-3 | 88.0% | 77.6% | 29.9% | 72.5% | 3.6s |
| FT + RAG top-5 | 88.1% | 78.4% | 25.3% | 70.7% | 3.9s |

### 5.4 Выводы

1. **RAG улучшает Base модель**: Citation Accuracy +6.8% (73.5% → 80.3%) при top-5
2. **RAG НЕ улучшает FT модель**: все метрики снижаются при добавлении RAG к FT
3. **FT > Base+RAG**: BERTScore 89.8% vs 83.2% (+6.6%), Latency 2.5s vs 11.4s (4.5x быстрее)
4. **Объяснение**: Fine-tuning на 56K Q&A парах уже интернализировал знания из обучающих данных. RAG подаёт модели ту же информацию, которую она уже выучила, добавляя шум в промпт

---

## 6. Эксперимент 4: Разбивка по доменам и языкам

### 6.1 По правовым доменам (FT модели)

| Домен | N | 4B CitAcc | 8B CitAcc | 14B CitAcc |
|-------|---|-----------|-----------|------------|
| administrative | 24 | 93.1% | 93.1% | 93.1% |
| criminal | 20 | 95.0% | 95.0% | 95.0% |
| land | 26 | 92.3% | 92.3% | 92.3% |
| tax | 90 | 85.6% | 84.8% | 86.5% |
| business | 20 | 83.0% | 80.0% | 83.0% |
| civil | 115 | 54.9% | 53.4% | 57.4% |
| labor | 43 | 63.2% | 61.2% | 58.5% |

**Наблюдение**: Civil и labor — самые сложные домены (низкая Citation Accuracy). Criminal и administrative — самые простые. Это связано с тем, что гражданское и трудовое право требуют более точных ссылок на конкретные статьи.

### 6.2 По языкам

| Модель | KZ BERTScore | KZ CitAcc | RU BERTScore | RU CitAcc |
|--------|-------------|-----------|-------------|-----------|
| 4B Base | 82.2% | 96.2% | 82.5% | 66.4% |
| 4B FT | 91.1% | 94.4% | 89.2% | 75.8% |
| 8B FT | 91.2% | 94.8% | 89.4% | 74.7% |
| 14B FT | 91.7% | 95.3% | 89.7% | 75.8% |

**Наблюдение**: Казахский язык показывает значительно более высокую Citation Accuracy (94-96% vs 74-76%). Это объясняется тем, что KZ вопросы в датасете обычно проще и короче, а также содержат меньше юридических ссылок.

---

## 7. Эксперимент 5: Подготовка экспертной оценки

### 7.1 Дизайн

- **50 вопросов** из валидационной выборки (random.seed(42))
- **4 модели**: Base 4B, FT 4B, FT 8B, FT 14B
- **200 ответов** (50 × 4), перемешаны и анонимизированы (A/B/C/D)
- **Критерии**: Корректность (1-5), Полнота (1-5), Релевантность (1-5), Галлюцинация (да/нет)

### 7.2 Подготовленные файлы

| Файл | Описание | Для кого |
|------|----------|----------|
| `human_eval/eval_sheet_*.csv` | Таблица оценки (русские заголовки, UTF-8 BOM) | Юристам |
| `human_eval/ИНСТРУКЦИЯ.txt` | Инструкция по оценке | Юристам |
| `human_eval/model_key_*.json` | Секретный ключ (модель → ответ) | НЕ юристам |

### 7.3 Статус
Листы подготовлены, ожидают оценки юристами.

---

## 8. Инструменты и воспроизводимость

### 8.1 Структура проекта

```
fineTuneQwen/
├── train.py                  # Единый скрипт обучения (--model 4b/8b/14b)
├── benchmark.py              # Бенчмарк с BERTScore, Citation Acc, CI
├── rag_benchmark.py          # RAG ablation (--model/--baseline)
├── compare_results.py        # Сводные таблицы scaling analysis
├── analyze_dataset.py        # Статистика датасета
├── prepare_human_eval.py     # Листы экспертной оценки
├── generate_report.py        # Генерация отчёта с графиками
├── config.py                 # Конфигурация (пути, LoRA, промпт)
├── utils.py                  # GPU info, format_dataset
├── combined_data/
│   ├── train.jsonl           # 56,802 обучающих пар
│   └── validation.jsonl      # 6,312 валидационных пар
├── results/                  # JSON результаты всех экспериментов
├── report/                   # Отчёт + графики
│   ├── experiment_report.md
│   └── figures/              # 5 PNG графиков
├── human_eval/               # Листы экспертной оценки
└── lora_qwen3_{4b,8b,14b}/  # Обученные адаптеры
```

### 8.2 Ключевые библиотеки

| Библиотека | Версия | Назначение |
|-----------|--------|------------|
| unsloth | 2025.12.1 / 2026.1.2 | 4-bit fine-tuning, inference |
| transformers | 4.56-4.57 | Модели, токенизация |
| trl | 0.12+ | SFTTrainer |
| torch | 2.9.1 | Фреймворк |
| bert-score | latest | BERTScore метрика |
| rouge-score | latest | ROUGE метрика |
| scikit-learn | latest | TF-IDF для RAG |
| matplotlib | latest | Графики |

### 8.3 Воспроизводимость

- `random.seed(42)` для выборки валидационных данных
- `np.random.seed(42)` для bootstrap CI
- Все результаты сохранены в JSON с timestamp
- Git: https://github.com/nurkal022/fine_tune_qwen3
- Все гиперпараметры зафиксированы в `config.py`

---

## 9. Хронология работ

| Дата | Что сделано |
|------|------------|
| Янв 2026 | Обучение 8B модели, первые бенчмарки |
| 15 Фев | Обучение 4B модели (15.5 часов, RTX 5080) |
| 16 Фев | Создание unified pipeline (train.py, benchmark.py) |
| 16 Фев | Бенчмарки 4B/8B (100 samples), обнаружена проблема с импортами |
| 17 Фев | Обучение 14B на сервере (14 часов, RTX 5090) |
| 17-18 Фев | Бенчмарки 14B на сервере |
| 18 Фев | Все бенчмарки перезапущены на 500 samples + bootstrap CI |
| 21 Фев | RAG ablation: FT + RAG (200 samples) |
| 21 Фев | RAG ablation: Base + RAG (200 samples, ~2.5 часа) |
| 21 Фев | Подготовка листов экспертной оценки (Эксперимент 5) |
| 21 Фев | Генерация отчёта с графиками |

---

## 10. Известные ограничения

1. **Одинаковый seed для всех бенчмарков**: одни и те же 500/200 вопросов → результаты сравнимы, но не показывают вариативность на разных подвыборках
2. **TF-IDF для RAG**: простой retrieval, не dense (sentence-transformers + FAISS). Результаты RAG могут улучшиться с лучшим retriever
3. **4-bit квантизация**: все модели в 4-bit, что может ограничивать потенциал больших моделей
4. **Citation Accuracy метрика**: regex-based, может пропускать некоторые форматы ссылок
5. **Hallucination Rate**: считает только юридические ссылки, не проверяет фактическую корректность утверждений
6. **Экспертная оценка**: ещё не проведена, результаты Эксперимента 5 отсутствуют

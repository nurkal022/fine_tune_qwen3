"""
–ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π —á–∞—Ç —Å –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª—å—é Qwen3-8B
"""
from unsloth import FastLanguageModel
from transformers import TextStreamer

MODEL_PATH = "finetuned_qwen3_8b"
MAX_SEQ_LENGTH = 2048

ALPACA_PROMPT = """Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.

### Instruction:
{}

### Input:
{}

### Response:
{}"""

def main():
    print("=" * 60)
    print("–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏...")
    print("=" * 60)
    
    model, tokenizer = FastLanguageModel.from_pretrained(
        model_name=MODEL_PATH,
        max_seq_length=MAX_SEQ_LENGTH,
        dtype=None,
        load_in_4bit=True,
    )
    FastLanguageModel.for_inference(model)
    
    print("‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞!\n")
    
    print("=" * 60)
    print("–ò–ù–¢–ï–†–ê–ö–¢–ò–í–ù–´–ô –ß–ê–¢")
    print("–í–≤–µ–¥–∏—Ç–µ 'exit' –∏–ª–∏ 'quit' –¥–ª—è –≤—ã—Ö–æ–¥–∞")
    print("=" * 60)
    
    streamer = TextStreamer(tokenizer, skip_prompt=True)
    
    while True:
        print("\n" + "-" * 60)
        instruction = input("üí¨ –í–∞—à –≤–æ–ø—Ä–æ—Å: ").strip()
        
        if instruction.lower() in ['exit', 'quit', '–≤—ã—Ö–æ–¥']:
            print("\n–î–æ —Å–≤–∏–¥–∞–Ω–∏—è!")
            break
        
        if not instruction:
            continue
        
        input_text = input("üìù –ö–æ–Ω—Ç–µ–∫—Å—Ç (Enter –¥–ª—è –ø—Ä–æ–ø—É—Å–∫–∞): ").strip()
        
        prompt = ALPACA_PROMPT.format(instruction, input_text, "")
        inputs = tokenizer([prompt], return_tensors="pt").to("cuda")
        
        print("\nü§ñ –û—Ç–≤–µ—Ç:")
        print("-" * 60)
        _ = model.generate(
            **inputs,
            streamer=streamer,
            max_new_tokens=512,
            use_cache=True,
            temperature=0.7,
            top_p=0.9,
        )
        print("-" * 60)

if __name__ == "__main__":
    main()


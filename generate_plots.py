#!/usr/bin/env python3
"""
–ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä –≥—Ä–∞—Ñ–∏–∫–æ–≤ –∏ –æ—Ç—á—ë—Ç–æ–≤ –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –æ–±—É—á–µ–Ω–∏—è Qwen3
"""

import json
import re
import os
from datetime import datetime

# –ü–æ–ø—Ä–æ–±—É–µ–º –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å matplotlib
try:
    import matplotlib.pyplot as plt
    import matplotlib
    matplotlib.use('Agg')  # –î–ª—è —Ä–∞–±–æ—Ç—ã –±–µ–∑ –¥–∏—Å–ø–ª–µ—è
    HAS_MATPLOTLIB = True
except ImportError:
    HAS_MATPLOTLIB = False
    print("‚ö†Ô∏è  matplotlib –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –ì—Ä–∞—Ñ–∏–∫–∏ –Ω–µ –±—É–¥—É—Ç —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω—ã.")
    print("   –£—Å—Ç–∞–Ω–æ–≤–∏: pip install matplotlib")

try:
    import numpy as np
    HAS_NUMPY = True
except ImportError:
    HAS_NUMPY = False


def parse_training_log(log_file: str = "log.txt"):
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ loss –∏–∑ –ª–æ–≥-—Ñ–∞–π–ª–∞"""
    losses = []
    epochs = []
    learning_rates = []
    
    if not os.path.exists(log_file):
        return None
    
    with open(log_file, 'r', encoding='utf-8') as f:
        for line in f:
            # –ü–∞—Ä—Å–∏–Ω–≥ —Å—Ç—Ä–æ–∫ –≤–∏–¥–∞: {'loss': 1.8544, 'grad_norm': ..., 'learning_rate': ..., 'epoch': 0.01}
            match = re.search(r"\{'loss': ([\d.]+).*'learning_rate': ([\d.e-]+).*'epoch': ([\d.]+)\}", line)
            if match:
                losses.append(float(match.group(1)))
                learning_rates.append(float(match.group(2)))
                epochs.append(float(match.group(3)))
    
    return {
        'losses': losses,
        'epochs': epochs,
        'learning_rates': learning_rates
    }


def load_benchmark_results(benchmark_file: str = None):
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –±–µ–Ω—á–º–∞—Ä–∫–∞"""
    if benchmark_file is None:
        # –ò—â–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π —Ñ–∞–π–ª –±–µ–Ω—á–º–∞—Ä–∫–∞
        files = [f for f in os.listdir('.') if f.startswith('benchmark_results_') and f.endswith('.json')]
        if not files:
            return None
        benchmark_file = sorted(files)[-1]
    
    with open(benchmark_file, 'r', encoding='utf-8') as f:
        return json.load(f)


def load_training_metrics(metrics_file: str = "logs/metrics_20260127_095201.json"):
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –º–µ—Ç—Ä–∏–∫–∏ –æ–±—É—á–µ–Ω–∏—è"""
    if not os.path.exists(metrics_file):
        return None
    with open(metrics_file, 'r', encoding='utf-8') as f:
        return json.load(f)


def load_training_config(config_file: str = "logs/config_20260127_095201.json"):
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –æ–±—É—á–µ–Ω–∏—è"""
    if not os.path.exists(config_file):
        return None
    with open(config_file, 'r', encoding='utf-8') as f:
        return json.load(f)


def generate_training_loss_plot(training_data: dict, output_dir: str = "plots"):
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –≥—Ä–∞—Ñ–∏–∫ loss –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è"""
    if not HAS_MATPLOTLIB or training_data is None:
        return None
    
    os.makedirs(output_dir, exist_ok=True)
    
    fig, ax = plt.subplots(figsize=(12, 6))
    
    epochs = training_data['epochs']
    losses = training_data['losses']
    
    ax.plot(epochs, losses, 'b-', alpha=0.3, linewidth=0.5, label='Loss (raw)')
    
    # –°–∫–æ–ª—å–∑—è—â–µ–µ —Å—Ä–µ–¥–Ω–µ–µ
    if HAS_NUMPY and len(losses) > 20:
        window = min(50, len(losses) // 10)
        smoothed = np.convolve(losses, np.ones(window)/window, mode='valid')
        smooth_epochs = epochs[window//2:window//2+len(smoothed)]
        ax.plot(smooth_epochs, smoothed, 'b-', linewidth=2, label=f'Loss (smoothed, window={window})')
    
    ax.set_xlabel('Epoch', fontsize=12)
    ax.set_ylabel('Loss', fontsize=12)
    ax.set_title('Training Loss - Qwen3-8B Fine-tuning', fontsize=14, fontweight='bold')
    ax.legend()
    ax.grid(True, alpha=0.3)
    
    # –î–æ–±–∞–≤–ª—è–µ–º –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏
    if losses:
        ax.annotate(f'Start: {losses[0]:.3f}', xy=(epochs[0], losses[0]), 
                   xytext=(10, 10), textcoords='offset points', fontsize=10)
        ax.annotate(f'End: {losses[-1]:.3f}', xy=(epochs[-1], losses[-1]), 
                   xytext=(-50, 10), textcoords='offset points', fontsize=10)
    
    plt.tight_layout()
    output_path = os.path.join(output_dir, 'training_loss.png')
    plt.savefig(output_path, dpi=150, bbox_inches='tight')
    plt.close()
    
    return output_path


def generate_learning_rate_plot(training_data: dict, output_dir: str = "plots"):
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –≥—Ä–∞—Ñ–∏–∫ learning rate"""
    if not HAS_MATPLOTLIB or training_data is None:
        return None
    
    os.makedirs(output_dir, exist_ok=True)
    
    fig, ax = plt.subplots(figsize=(12, 4))
    
    epochs = training_data['epochs']
    lrs = training_data['learning_rates']
    
    ax.plot(epochs, lrs, 'g-', linewidth=1.5)
    ax.set_xlabel('Epoch', fontsize=12)
    ax.set_ylabel('Learning Rate', fontsize=12)
    ax.set_title('Learning Rate Schedule', fontsize=14, fontweight='bold')
    ax.grid(True, alpha=0.3)
    ax.ticklabel_format(style='scientific', axis='y', scilimits=(0,0))
    
    plt.tight_layout()
    output_path = os.path.join(output_dir, 'learning_rate.png')
    plt.savefig(output_path, dpi=150, bbox_inches='tight')
    plt.close()
    
    return output_path


def generate_benchmark_metrics_plot(benchmark_data: dict, output_dir: str = "plots"):
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –≥—Ä–∞—Ñ–∏–∫ –º–µ—Ç—Ä–∏–∫ –±–µ–Ω—á–º–∞—Ä–∫–∞"""
    if not HAS_MATPLOTLIB or benchmark_data is None:
        return None
    
    os.makedirs(output_dir, exist_ok=True)
    
    metrics = benchmark_data['avg_metrics']
    
    # –û—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
    fig, axes = plt.subplots(1, 2, figsize=(14, 5))
    
    # 1. Bar chart –º–µ—Ç—Ä–∏–∫
    metric_names = ['Token F1', 'Exact Match', 'Key Info', 'Length Ratio']
    metric_values = [
        metrics['token_f1'] * 100,
        metrics['exact_match'] * 100,
        metrics['key_info'] * 100,
        metrics['length_ratio'] * 100
    ]
    colors = ['#3498db', '#e74c3c', '#2ecc71', '#f39c12']
    
    bars = axes[0].bar(metric_names, metric_values, color=colors, edgecolor='black', linewidth=1.2)
    axes[0].set_ylabel('Score (%)', fontsize=12)
    axes[0].set_title('Benchmark Metrics - Qwen3-8B', fontsize=14, fontweight='bold')
    axes[0].set_ylim(0, 100)
    axes[0].grid(True, alpha=0.3, axis='y')
    
    # –î–æ–±–∞–≤–ª—è–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –Ω–∞ —Å—Ç–æ–ª–±—Ü—ã
    for bar, val in zip(bars, metric_values):
        axes[0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1, 
                    f'{val:.1f}%', ha='center', va='bottom', fontsize=11, fontweight='bold')
    
    # 2. –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ Token F1 –ø–æ –ø—Ä–∏–º–µ—Ä–∞–º
    if 'detailed_results' in benchmark_data:
        f1_scores = [r['metrics']['token_f1'] * 100 for r in benchmark_data['detailed_results']]
        
        axes[1].hist(f1_scores, bins=20, color='#3498db', edgecolor='black', alpha=0.7)
        axes[1].axvline(x=sum(f1_scores)/len(f1_scores), color='red', linestyle='--', 
                       linewidth=2, label=f'Mean: {sum(f1_scores)/len(f1_scores):.1f}%')
        axes[1].set_xlabel('Token F1 Score (%)', fontsize=12)
        axes[1].set_ylabel('Count', fontsize=12)
        axes[1].set_title('Token F1 Distribution', fontsize=14, fontweight='bold')
        axes[1].legend()
        axes[1].grid(True, alpha=0.3)
    
    plt.tight_layout()
    output_path = os.path.join(output_dir, 'benchmark_metrics.png')
    plt.savefig(output_path, dpi=150, bbox_inches='tight')
    plt.close()
    
    return output_path


def generate_key_info_distribution(benchmark_data: dict, output_dir: str = "plots"):
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ Key Info Score"""
    if not HAS_MATPLOTLIB or benchmark_data is None or 'detailed_results' not in benchmark_data:
        return None
    
    os.makedirs(output_dir, exist_ok=True)
    
    key_info_scores = [r['metrics']['key_info'] * 100 for r in benchmark_data['detailed_results']]
    
    fig, ax = plt.subplots(figsize=(10, 5))
    
    # –ö–∞—Ç–µ–≥–æ—Ä–∏–∏
    labels = ['0-25%', '25-50%', '50-75%', '75-100%']
    colors = ['#e74c3c', '#f39c12', '#3498db', '#2ecc71']
    
    counts = [
        sum(1 for s in key_info_scores if 0 <= s < 25),
        sum(1 for s in key_info_scores if 25 <= s < 50),
        sum(1 for s in key_info_scores if 50 <= s < 75),
        sum(1 for s in key_info_scores if 75 <= s <= 100)
    ]
    
    bars = ax.bar(labels, counts, color=colors, edgecolor='black', linewidth=1.2)
    ax.set_ylabel('Number of Examples', fontsize=12)
    ax.set_xlabel('Key Info Score Range', fontsize=12)
    ax.set_title('Key Information Preservation Distribution', fontsize=14, fontweight='bold')
    ax.grid(True, alpha=0.3, axis='y')
    
    for bar, val in zip(bars, counts):
        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5, 
               str(val), ha='center', va='bottom', fontsize=12, fontweight='bold')
    
    plt.tight_layout()
    output_path = os.path.join(output_dir, 'key_info_distribution.png')
    plt.savefig(output_path, dpi=150, bbox_inches='tight')
    plt.close()
    
    return output_path


def generate_generation_time_plot(benchmark_data: dict, output_dir: str = "plots"):
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –≥—Ä–∞—Ñ–∏–∫ –≤—Ä–µ–º–µ–Ω–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏"""
    if not HAS_MATPLOTLIB or benchmark_data is None or 'detailed_results' not in benchmark_data:
        return None
    
    os.makedirs(output_dir, exist_ok=True)
    
    gen_times = [r['metrics']['gen_time'] for r in benchmark_data['detailed_results']]
    
    fig, ax = plt.subplots(figsize=(10, 5))
    
    ax.hist(gen_times, bins=30, color='#9b59b6', edgecolor='black', alpha=0.7)
    ax.axvline(x=sum(gen_times)/len(gen_times), color='red', linestyle='--', 
               linewidth=2, label=f'Mean: {sum(gen_times)/len(gen_times):.2f}s')
    ax.set_xlabel('Generation Time (seconds)', fontsize=12)
    ax.set_ylabel('Count', fontsize=12)
    ax.set_title('Response Generation Time Distribution', fontsize=14, fontweight='bold')
    ax.legend()
    ax.grid(True, alpha=0.3)
    
    plt.tight_layout()
    output_path = os.path.join(output_dir, 'generation_time.png')
    plt.savefig(output_path, dpi=150, bbox_inches='tight')
    plt.close()
    
    return output_path


def generate_all_plots():
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –≤—Å–µ –≥—Ä–∞—Ñ–∏–∫–∏"""
    print("=" * 60)
    print("üìä –ì–ï–ù–ï–†–ê–¶–ò–Ø –ì–†–ê–§–ò–ö–û–í –ò –û–¢–ß–Å–¢–û–í")
    print("=" * 60)
    
    plots_generated = []
    
    # 1. –î–∞–Ω–Ω—ã–µ –æ–±—É—á–µ–Ω–∏—è
    print("\nüìà –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö –æ–±—É—á–µ–Ω–∏—è...")
    training_data = parse_training_log("log.txt")
    if training_data and training_data['losses']:
        print(f"   –ù–∞–π–¥–µ–Ω–æ {len(training_data['losses'])} –∑–∞–ø–∏—Å–µ–π loss")
        
        plot = generate_training_loss_plot(training_data)
        if plot:
            plots_generated.append(plot)
            print(f"   ‚úÖ {plot}")
        
        plot = generate_learning_rate_plot(training_data)
        if plot:
            plots_generated.append(plot)
            print(f"   ‚úÖ {plot}")
    else:
        print("   ‚ö†Ô∏è  –î–∞–Ω–Ω—ã–µ –æ–±—É—á–µ–Ω–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
    
    # 2. –î–∞–Ω–Ω—ã–µ –±–µ–Ω—á–º–∞—Ä–∫–∞
    print("\nüìä –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –±–µ–Ω—á–º–∞—Ä–∫–∞...")
    benchmark_data = load_benchmark_results()
    if benchmark_data:
        print(f"   –ú–æ–¥–µ–ª—å: {benchmark_data['model_path']}")
        print(f"   –ü—Ä–∏–º–µ—Ä–æ–≤: {benchmark_data['num_samples']}")
        
        plot = generate_benchmark_metrics_plot(benchmark_data)
        if plot:
            plots_generated.append(plot)
            print(f"   ‚úÖ {plot}")
        
        plot = generate_key_info_distribution(benchmark_data)
        if plot:
            plots_generated.append(plot)
            print(f"   ‚úÖ {plot}")
        
        plot = generate_generation_time_plot(benchmark_data)
        if plot:
            plots_generated.append(plot)
            print(f"   ‚úÖ {plot}")
    else:
        print("   ‚ö†Ô∏è  –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –±–µ–Ω—á–º–∞—Ä–∫–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
    
    # 3. –ò—Ç–æ–≥–∏
    print("\n" + "=" * 60)
    if plots_generated:
        print(f"‚úÖ –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ {len(plots_generated)} –≥—Ä–∞—Ñ–∏–∫–æ–≤ –≤ –ø–∞–ø–∫–µ 'plots/'")
    else:
        print("‚ö†Ô∏è  –ì—Ä–∞—Ñ–∏–∫–∏ –Ω–µ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω—ã (matplotlib –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω?)")
    
    return plots_generated


def generate_summary_stats():
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Å–≤–æ–¥–Ω—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É"""
    stats = {}
    
    # Training config
    config = load_training_config()
    if config:
        stats['config'] = config
    
    # Training metrics
    metrics = load_training_metrics()
    if metrics:
        stats['training'] = metrics
    
    # Benchmark results
    benchmark = load_benchmark_results()
    if benchmark:
        stats['benchmark'] = {
            'model_path': benchmark['model_path'],
            'num_samples': benchmark['num_samples'],
            'total_time': benchmark['total_time'],
            'metrics': benchmark['avg_metrics']
        }
    
    # Training log stats
    training_data = parse_training_log("log.txt")
    if training_data and training_data['losses']:
        losses = training_data['losses']
        stats['loss_stats'] = {
            'initial_loss': losses[0],
            'final_loss': losses[-1],
            'min_loss': min(losses),
            'max_loss': max(losses),
            'loss_reduction': (losses[0] - losses[-1]) / losses[0] * 100,
            'total_steps': len(losses)
        }
    
    return stats


if __name__ == "__main__":
    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –≥—Ä–∞—Ñ–∏–∫–∏
    plots = generate_all_plots()
    
    # –í—ã–≤–æ–¥–∏–º —Å–≤–æ–¥–∫—É
    print("\n" + "=" * 60)
    print("üìã –°–í–û–î–ù–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê")
    print("=" * 60)
    
    stats = generate_summary_stats()
    
    if 'config' in stats:
        c = stats['config']
        print(f"\nüîß –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è:")
        print(f"   –ú–æ–¥–µ–ª—å: {c.get('model_name', 'N/A')}")
        print(f"   Sequence length: {c.get('max_seq_length', 'N/A')}")
        print(f"   LoRA rank: {c.get('lora_r', 'N/A')}")
        print(f"   Batch size: {c.get('effective_batch', 'N/A')}")
        print(f"   Epochs: {c.get('num_epochs', 'N/A')}")
        print(f"   Learning rate: {c.get('learning_rate', 'N/A')}")
    
    if 'training' in stats:
        t = stats['training']
        runtime_hours = t.get('train_runtime', 0) / 3600
        print(f"\n‚è±Ô∏è  –û–±—É—á–µ–Ω–∏–µ:")
        print(f"   –í—Ä–µ–º—è: {runtime_hours:.2f} —á–∞—Å–æ–≤")
        print(f"   Final loss: {t.get('train_loss', 'N/A'):.4f}")
        print(f"   Epochs completed: {t.get('epoch', 'N/A')}")
    
    if 'loss_stats' in stats:
        l = stats['loss_stats']
        print(f"\nüìâ Loss —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
        print(f"   –ù–∞—á–∞–ª—å–Ω—ã–π: {l['initial_loss']:.4f}")
        print(f"   –ö–æ–Ω–µ—á–Ω—ã–π: {l['final_loss']:.4f}")
        print(f"   –°–Ω–∏–∂–µ–Ω–∏–µ: {l['loss_reduction']:.1f}%")
        print(f"   –ú–∏–Ω–∏–º—É–º: {l['min_loss']:.4f}")
    
    if 'benchmark' in stats:
        b = stats['benchmark']
        m = b['metrics']
        print(f"\nüß™ –ë–µ–Ω—á–º–∞—Ä–∫ ({b['num_samples']} –ø—Ä–∏–º–µ—Ä–æ–≤):")
        print(f"   Token F1: {m['token_f1']*100:.2f}%")
        print(f"   Exact Match: {m['exact_match']*100:.2f}%")
        print(f"   Key Info: {m['key_info']*100:.2f}%")
        print(f"   Length Ratio: {m['length_ratio']*100:.2f}%")
        print(f"   Avg gen time: {m['gen_time']:.2f}s")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–≤–æ–¥–∫—É –≤ JSON
    summary_file = f"training_summary_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    with open(summary_file, 'w', encoding='utf-8') as f:
        json.dump(stats, f, indent=2, ensure_ascii=False)
    print(f"\nüíæ –°–≤–æ–¥–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {summary_file}")

